{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c70448",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dbe4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple Linear Regression:\n",
    "    \n",
    "Simple linear regression involves predicting the values of one variable (dependent variable) based on the values of another\n",
    "variable (independent variable). The relationship between the two variables is assumed to be linear, meaning it can be \n",
    "represented by a straight line.\n",
    "\n",
    "Example:\n",
    "Let's say we want to predict a student's exam score (y) based on the number of hours they studied (x). A simple linear \n",
    "regression model might try to fit a line to these data points to predict the exam score based on the number of hours studied.\n",
    "\n",
    "\n",
    "Multiple Linear Regression:\n",
    "Multiple linear regression extends the concept of simple linear regression to multiple independent variables. Instead of \n",
    "predicting y based on just one x, we predict y based on multiple variables 1,2,x1,x2,......xn.\n",
    "\n",
    "Example:\n",
    "Suppose we want to predict a person's income (y) based on their education level (x1), years of work experience (x2), and the \n",
    "city's cost of living index (x3). A multiple linear regression model would consider all these variables to make predictions \n",
    "about the income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear regression makes several assumptions about the data for the model to be valid. \n",
    "It's important to check these assumptions to ensure the reliability of the regression results. \n",
    "The key assumptions of linear regression are:\n",
    "\n",
    "Linearity: The relationship between the dependent and independent variables should be linear. You can check this assumption by\n",
    "plotting the data and visually inspecting if the relationship appears to be reasonably linear. Additionally, residual plots \n",
    "can be used to identify patterns that might suggest non-linearity.\n",
    "\n",
    "Independence of Residuals: The residuals (the differences between the observed and predicted values) should be independent of \n",
    "each other. This assumption is crucial because if there is a pattern in the residuals, it suggests that the model is missing \n",
    "some information. A scatter plot of residuals against the predicted values can help identify any patterns or trends.\n",
    "\n",
    "Homoscedasticity (Constant Variance of Residuals): The variance of the residuals should be constant across all levels of the \n",
    "independent variables. A scatter plot of residuals against predicted values can be used to check for homoscedasticity. \n",
    "If the spread of residuals widens or narrows systematically as the predicted values increase, there may be an issue.\n",
    "\n",
    "Normality of Residuals: The residuals should be approximately normally distributed. This assumption is more critical for \n",
    "smaller sample sizes. You can use a histogram of residuals or a Q-Q plot to assess normality. If the residuals deviate \n",
    "significantly from a normal distribution, it might be an indication that the model is not the best fit for the data.\n",
    "\n",
    "No Perfect Multicollinearity: In the case of multiple linear regression, the independent variables should not be perfectly \n",
    "correlated with each other. Multicollinearity can lead to unstable coefficient estimates. Variance Inflation Factor (VIF) \n",
    "values can be calculated to assess multicollinearity; high VIF values indicate potential multicollinearity issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212ced39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae289b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Slope (Coefficient of the Independent Variable): The slope (often denoted as b) represents the change in the dependent \n",
    "variable for a one-unit change in the independent variable, assuming all other variables are held constant. A positive \n",
    "slope indicates a positive relationship, and a negative slope indicates a negative relationship.\n",
    "\n",
    "Intercept: The intercept (often denoted as b0) is the value of the dependent variable when all independent variables are zero.\n",
    "It is the starting point of the regression line.\n",
    "\n",
    "Example:\n",
    "Suppose we have a linear regression model predicting a person's salary (y) based on years of experience (x). If the slope (b)\n",
    "is 2000, it means that, on average, each additional year of experience is associated with a 2000 increase in salary. \n",
    "The intercept (b0) might be 30,000, indicating that someone with zero years of experience is estimated to have a starting \n",
    "salary of 30,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc6eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb6812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Descent is an optimization algorithm used to minimize the cost function in machine learning models.\n",
    "It iteratively adjusts the model parameters to find the minimum of the cost function. The algorithm calculates the gradient of\n",
    "the cost function with respect to the model parameters and updates the parameters in the direction that reduces the cost. \n",
    "This process continues until the algorithm converges to a minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f431f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fed7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiple linear regression is an extension of simple linear regression that involves more than one independent variable.\n",
    "\n",
    "y=b0+b1x1+b2x2+...+bnxn\n",
    "\n",
    "The key difference from simple linear regression is that it considers multiple independent variables, allowing for a \n",
    "more comprehensive analysis of how different factors contribute to the variation in the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e72d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multicollinearity occurs when independent variables in a multiple linear regression model are highly correlated, \n",
    "making it challenging to isolate the individual effect of each variable. This can lead to unstable coefficient estimates.\n",
    "\n",
    "Detection:\n",
    "\n",
    "Calculate the Variance Inflation Factor (VIF) for each independent variable. High VIF values (typically above 10) \n",
    "indicate potential multicollinearity issues.\n",
    "\n",
    "Addressing:\n",
    "\n",
    "Remove one of the highly correlated variables.\n",
    "Combine correlated variables into a single variable.\n",
    "Use regularization techniques like Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f5bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf8550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Polynomial regression is a type of regression analysis where the relationship between the independent variable x and the \n",
    "dependent variable y is modeled as an nth-degree polynomial. The equation for a polynomial regression of degree n is:\n",
    "    \n",
    "y=b0+b1x+b2x2+...+bnxn\n",
    "\n",
    "It allows for a more flexible fit to the data compared to linear regression, capturing non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1bdca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages:\n",
    "\n",
    "Captures non-linear relationships in the data.\n",
    "More flexible than linear regression.\n",
    "Disadvantages:\n",
    "\n",
    "Prone to overfitting, especially with higher-degree polynomials.\n",
    "Interpretability can be challenging with higher-degree terms.\n",
    "Increased complexity in model selection and tuning.\n",
    "When to use Polynomial Regression:\n",
    "\n",
    "When there is a clear non-linear relationship between variables.\n",
    "When the underlying data distribution is better represented by a polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f293e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
