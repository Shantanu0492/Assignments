{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfe94b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "R-squared (R²) is a statistical measure that represents the proportion of the variance in the dependent variable that \n",
    "is predictable from the independent variables in a linear regression model. In simpler terms, it indicates the goodness\n",
    "of fit of the model. The value of R-squared ranges from 0 to 1, where 0 indicates that the model does not explain any \n",
    "variability in the dependent variable, and 1 indicates that the model explains all the variability.\n",
    "\n",
    "Mathematically, R-squared is calculated as follows:\n",
    "    \n",
    "R**2=1−SST/SSR\n",
    "\n",
    "SSR is the sum of squared residuals (the difference between the predicted values and the actual values).\n",
    "SST is the total sum of squares, which represents the total variability in the dependent variable.\n",
    "\n",
    "Interpreting R-squared\n",
    "\n",
    "A higher R-squared value indicates a better fit of the model to the data.\n",
    "A low R-squared value suggests that the model may not be capturing much of the variability in the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d98824",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjusted R-squared is a modification of the regular R-squared that accounts for the number of predictors in a linear \n",
    "regression model. While R-squared measures the proportion of the variance in the dependent variable explained by the \n",
    "independent variables, adjusted R-squared adjusts this value based on the number of predictors in the model. The purpose \n",
    "is to provide a more accurate assessment of the model's goodness of fit, especially when comparing models with different\n",
    "numbers of predictors.\n",
    "\n",
    "The adjustment penalizes the model for including additional predictors that do not significantly improve the model's \n",
    "explanatory power. As the number of predictors increases, the adjusted R-squared will only increase if the new predictors \n",
    "contribute enough to offset the penalty for their inclusion.\n",
    "\n",
    "Key differences between R-squared and adjusted R-squared\n",
    "\n",
    "Penalty for Additional Predictors: Adjusted R-squared incorporates a penalty for including more predictors in the model, \n",
    "discouraging the inclusion of variables that do not significantly improve the model's fit.\n",
    "\n",
    "Comparability: Adjusted R-squared is particularly useful when comparing models with different numbers of predictors. \n",
    "It helps identify whether the inclusion of additional variables is justified by a significant improvement in explanatory power.\n",
    "\n",
    "Magnitude: In general, adjusted R-squared will be lower than R-squared, especially when there are many predictors in the model.\n",
    "\n",
    "While adjusted R-squared provides a more nuanced assessment of model fit, it is essential to consider both metrics along \n",
    "with other diagnostics when evaluating regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d2ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097bba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjusted R-squared is more appropriate to use when you are comparing regression models with different numbers of predictors.\n",
    "Here are some specific situations where adjusted R-squared is particularly useful:\n",
    "    \n",
    "Model Comparison: Adjusted R-squared is valuable when comparing multiple regression models with varying numbers of predictors.\n",
    "It helps you assess whether the inclusion of additional variables in a more complex model is justified by a significant \n",
    "improvement in explanatory power.\n",
    "\n",
    "Variable Selection: If you are in the process of variable selection, aiming to identify the most relevant predictors for your\n",
    "model, adjusted R-squared can guide you by penalizing the addition of variables that do not contribute substantially to the \n",
    "model's explanatory power.\n",
    "\n",
    "Preventing Overfitting: Adjusted R-squared helps guard against overfitting, which occurs when a model fits the training data \n",
    "too closely, capturing noise rather than the underlying patterns. The penalty for additional predictors discourages the \n",
    "inclusion of variables that may lead to overfitting.\n",
    "\n",
    "Complex Models: In situations where you have a relatively large number of potential predictors, using adjusted R-squared \n",
    "can be more informative than relying solely on the regular R-squared. It provides a more accurate measure of the model's \n",
    "performance, considering both fit and model complexity.\n",
    "\n",
    "Regression Analysis with Automatic Variable Selection: When using techniques that automatically select variables \n",
    "(e.g., stepwise regression), adjusted R-squared is often preferred over R-squared to guide the selection process, as it \n",
    "accounts for the trade-off between model fit and complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4ab555",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3d03e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the context of regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error)\n",
    "are commonly used metrics to evaluate the performance of a regression model by measuring the accuracy of its predictions \n",
    "against the actual values.\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "\n",
    "MAE represents the average absolute difference between the predicted and actual values. It is calculated as the mean of \n",
    "the absolute values of the residuals (the differences between predicted and actual values).\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "\n",
    "MSE represents the average squared difference between the predicted and actual values. It is calculated as the mean of the \n",
    "squared residuals.\n",
    "MSE penalizes larger errors more heavily than smaller errors because of the squaring operation. Like MAE, it is a measure \n",
    "of the average magnitude of the errors.\n",
    "\n",
    "Root Mean Squared Error (RMSE):\n",
    "\n",
    "RMSE is the square root of the MSE and provides a measure of the typical magnitude of the errors. It is often preferred when \n",
    "you want the error metric to be in the same units as the dependent variable.\n",
    "\n",
    "RMSE is more sensitive to large errors compared to MAE and provides a clearer indication of how well the model is performing \n",
    "in terms of prediction accuracy.\n",
    "\n",
    "When choosing between these metrics, it depends on the specific context and the desired properties of the evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ffc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376eb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages and Disadvantages of RMSE, MSE, and MAE in Regression Analysis\n",
    "\n",
    "Mean Absolute Error (MAE)\n",
    "\n",
    "Advantages\n",
    "\n",
    "MAE is straightforward to interpret, representing the average absolute difference between predicted and actual values.\n",
    "It is less sensitive to outliers than MSE and RMSE, making it a robust metric in the presence of extreme values.\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "Since MAE treats all errors equally, it may not be suitable when large errors need to be penalized more severely.\n",
    "MAE does not provide a clear measure of the scale of errors, making it less intuitive for understanding the overall \n",
    "performance\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "\n",
    "Advantages:\n",
    "\n",
    "MSE penalizes larger errors more heavily than smaller errors, making it suitable when the emphasis is on minimizing \n",
    "significant deviations.\n",
    "Squaring the errors amplifies the impact of outliers, which can be useful in certain contexts.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "The squared nature of MSE can make it sensitive to outliers, leading to a potential distortion in the assessment of \n",
    "model performance.\n",
    "Since MSE is in squared units, it may not be directly interpretable in the same units as the dependent variable.\n",
    "Root Mean Squared Error (RMSE):\n",
    "\n",
    "Advantages:\n",
    "\n",
    "RMSE addresses the unit issue present in MSE, providing a metric in the same units as the dependent variable.\n",
    "It is sensitive to large errors, making it suitable for situations where significant deviations should be penalized.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Like MSE, RMSE can be heavily influenced by outliers, potentially skewing the evaluation of the model's overall performance.\n",
    "The square root operation can make RMSE less interpretable compared to MAE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac499759",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c614b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso Regularization:\n",
    "\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression to prevent \n",
    "overfitting and feature selection by adding a penalty term to the linear regression cost function. The penalty term is based\n",
    "on the absolute values of the coefficients.\n",
    "\n",
    "The inclusion of the absolute values of the coefficients in the penalty term encourages sparsity in the model, meaning that\n",
    "it tends to drive some of the coefficients to exactly zero. As a result, Lasso not only helps in preventing overfitting but\n",
    "also performs automatic feature selection by shrinking some coefficients to zero.\n",
    "\n",
    "Differences from Ridge Regularization:\n",
    "\n",
    "While both Lasso and Ridge regularization methods aim to prevent overfitting, they differ in the type of penalty term used:\n",
    "\n",
    "L1 Regularization (Lasso):\n",
    "    \n",
    "Promotes sparsity by driving some coefficients to exactly zero.\n",
    "Suitable for situations where there is a belief that only a subset of features is essential.\n",
    "\n",
    "L2 Regularization (Ridge)\n",
    "\n",
    "Does not drive coefficients to zero but penalizes them proportionally to their squared values.\n",
    "Suitable when all features are considered important, but some of them might have small coefficients.\n",
    "\n",
    "When to Use Lasso Regularization:\n",
    "\n",
    "Lasso regularization is more appropriate when:\n",
    "\n",
    "Feature selection is desired, and there is a belief that only a subset of features is relevant.\n",
    "The dataset has a large number of features, and it is suspected that many of them may not contribute significantly to the \n",
    "predictive power.\n",
    "Interpretability is important, as Lasso tends to produce sparse models with fewer non-zero coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd1e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307c7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularized linear models help prevent overfitting in machine learning by adding a penalty term to the cost function, \n",
    "discouraging overly complex models with large coefficients. This penalty term is based on the magnitudes of the model's\n",
    "coefficients, and it helps to strike a balance between fitting the training data well and avoiding overfitting\n",
    "\n",
    "Example: Ridge Regression (L2 Regularization)\n",
    "Consider a linear regression problem where you are predicting housing prices based on various features like square footage, \n",
    "number of bedrooms, and distance to the city center. The Ridge regression model aims to minimize the following cost function\n",
    "\n",
    "How Regularization Helps Prevent Overfitting:\n",
    "\n",
    "Penalizing Large Coefficients: The regularization term penalizes large coefficients by adding the sum of their squared values \n",
    "to the cost function. This encourages the model to keep the coefficients small.\n",
    "\n",
    "Balancing Model Complexity: The model aims to minimize both the prediction error (MSE) and the regularization term. As a \n",
    "result, it seeks a balance between fitting the training data well and avoiding overfitting by keeping the model parameters\n",
    "within reasonable bounds.\n",
    "\n",
    "Reducing Variance: By preventing the coefficients from becoming excessively large, regularization reduces the variance of\n",
    "the model, making it less sensitive to noise in the training data.\n",
    "\n",
    "Improved Generalization: Regularized models are more likely to generalize well to new, unseen data because they are less \n",
    "likely to memorize noise in the training set.\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume X_train, X_test, y_train, y_test are your training and testing data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "alpha = 1.0  # Regularization strength\n",
    "\n",
    "ridge_reg = Ridge(alpha=alpha)\n",
    "ridge_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = ridge_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error (MSE): {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c90acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this example, scaling the features is important when using regularization methods like Ridge, and the regularization \n",
    "strength (alpha) can be tuned based on cross-validation.\n",
    "\n",
    "Regularized linear models, such as Ridge and Lasso, provide a powerful tool to prevent overfitting and improve the robustness\n",
    "and generalization of machine learning models. The choice between Ridge and Lasso regularization depends on the specific \n",
    "characteristics of the problem and the desired properties of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96522fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21231762",
   "metadata": {},
   "outputs": [],
   "source": [
    "While regularized linear models, such as Ridge and Lasso regression, offer valuable tools for preventing overfitting and \n",
    "improving the robustness of linear models, they are not always the best choice for every regression analysis. \n",
    "\n",
    "Here are some limitations and reasons why regularized linear models may not be the optimal choice in certain situations\n",
    "\n",
    "Feature Selection Limitations:\n",
    "\n",
    "Lasso regression performs automatic feature selection by driving some coefficients to exactly zero. However, this can be a \n",
    "limitation when all features are genuinely important for the problem. In such cases, Ridge regression or non-regularized \n",
    "models might be more appropriate.\n",
    "\n",
    "Difficulty in Interpreting Coefficients:\n",
    "\n",
    "Regularization methods, particularly Lasso, may result in sparse models with some coefficients set to zero. \n",
    "While this is beneficial for feature selection, it can make interpreting the coefficients of the remaining features more\n",
    "challenging.\n",
    "\n",
    "Sensitivity to Scaling:\n",
    "\n",
    "Regularization techniques are sensitive to the scale of the features. It is essential to scale the features properly \n",
    "before applying regularization, especially for methods like Ridge that involve the sum of squared coefficients.\n",
    "\n",
    "Selection of Regularization Parameter:\n",
    "\n",
    "The choice of the regularization parameter (e.g.α in Ridge or Lasso) is crucial. Selecting an inappropriate value for the\n",
    "regularization parameter may lead to suboptimal model performance. Grid search or cross-validation is often used to find \n",
    "an optimal value, but this process can be computationally expensive.\n",
    "\n",
    "Loss of Information:\n",
    "\n",
    "Regularization imposes a penalty on the magnitude of the coefficients, which can lead to an underestimation of the true \n",
    "effect of certain predictors. In situations where understanding the precise impact of each predictor is crucial, a \n",
    "non-regularized model might be preferred.\n",
    "\n",
    "Assumption of Linearity:\n",
    "\n",
    "Regularized linear models assume a linear relationship between the predictors and the response variable. If the true \n",
    "relationship is highly non-linear, other techniques such as decision trees or kernelized methods may be more suitable.\n",
    "\n",
    "Not Suitable for Every Dataset:\n",
    "\n",
    "Regularization is particularly useful when dealing with datasets with a large number of features or when multicollinearity\n",
    "is present. For simpler datasets with fewer predictors, non-regularized linear regression models may perform well without \n",
    "the need for regularization.\n",
    "\n",
    "Elastic Net Trade-Offs:\n",
    "\n",
    "Elastic Net, a combination of Lasso and Ridge, mitigates some limitations of each, but it introduces an additional \n",
    "hyperparameter. Finding the right balance between L1 and L2 regularization can be challenging.\n",
    "\n",
    "Data Requirements:\n",
    "\n",
    "Regularization methods often perform better when there is a substantial amount of data available. In situations with limited \n",
    "data, regularization may not provide significant benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c3d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "The choice between Model A and Model B as the better performer depends on the specific context and goals of the analysis. \n",
    "Both RMSE (Root Mean Squared Error) and MAE (Mean Absolute Error) are metrics commonly used to evaluate the accuracy of \n",
    "regression models, but they emphasize different aspects of model performance.\n",
    "\n",
    "Comparing Model A (RMSE: 10) and Model B (MAE: 8)\n",
    "\n",
    "Model A (RMSE: 10):\n",
    "\n",
    "RMSE takes into account both the magnitude and direction of errors. It penalizes larger errors more heavily than smaller \n",
    "errors due to the squared term in its calculation.\n",
    "A lower RMSE suggests that, on average, the model's predictions have a smaller overall magnitude of error.\n",
    "\n",
    "Model B (MAE: 8):\n",
    "\n",
    "MAE focuses solely on the absolute magnitude of errors. It treats all errors equally, regardless of their size or direction.\n",
    "A lower MAE indicates that, on average, the model's predictions deviate less from the actual values in an absolute sense.\n",
    "\n",
    "Considerations:\n",
    "\n",
    "If the context of the problem emphasizes the importance of large errors and the consequences of getting predictions \n",
    "significantly wrong, then Model A with a lower RMSE might be preferred.\n",
    "\n",
    "If the focus is on overall prediction accuracy and minimizing the impact of outliers or extreme errors, Model B with a lower \n",
    "MAE might be favored.\n",
    "\n",
    "Limitations of the Metrics:\n",
    "\n",
    "Sensitivity to Outliers: Both RMSE and MAE are sensitive to outliers, but RMSE tends to be more influenced by large errors \n",
    "due to the squaring operation. If the dataset has extreme values, the choice of metric can significantly impact the \n",
    "evaluation.\n",
    "\n",
    "Interpretability: The interpretation of the \"better\" model depends on the specific goals of the analysis. While both metrics \n",
    "provide information about prediction accuracy, they might lead to different conclusions depending on the priorities of the \n",
    "task.\n",
    "\n",
    "Trade-off Between Precision and Robustness: RMSE places more emphasis on precision, penalizing larger errors more heavily. \n",
    "MAE is generally more robust to extreme errors but may sacrifice some precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d8c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "The choice between Ridge and Lasso regularization, as well as the specific regularization parameter values, depends on the \n",
    "characteristics of the data, the goals of the analysis, and the trade-offs associated with each type of regularization.\n",
    "\n",
    "Model A (Ridge Regularization with α=0.1)\n",
    "\n",
    "Ridge regularization adds a penalty term to the cost function based on the sum of squared coefficients\n",
    "A smaller α value (0.1 in this case) indicates a relatively mild regularization, allowing the model to have larger \n",
    "coefficients.\n",
    "\n",
    "Model B (Lasso Regularization with α=0.5)\n",
    "\n",
    "Lasso regularization adds a penalty term based on the sum of absolute values of coefficients\n",
    "\n",
    "Lasso tends to produce sparse models with some coefficients set exactly to zero, promoting feature selection.\n",
    "A larger α value (0.5 in this case) indicates a stronger regularization, increasing the likelihood of coefficients being \n",
    "exactly zero.\n",
    "\n",
    "Considerations for Choosing the Better Performer:\n",
    "\n",
    "Ridge vs. Lasso Trade-Offs:\n",
    "\n",
    "Ridge tends to shrink coefficients towards zero but rarely exactly to zero, maintaining all features in the model.\n",
    "Lasso can drive some coefficients exactly to zero, effectively performing feature selection.\n",
    "If feature selection is essential, and you believe that some features are not relevant, Lasso might be preferred.\n",
    "\n",
    "Impact of Regularization Strength (α):\n",
    "\n",
    "A smaller α value allows for less regularization, potentially leading to models with larger coefficients.\n",
    "A larger α value increases the strength of regularization, pushing coefficients towards zero and promoting sparsity in Lasso.\n",
    "\n",
    "Interpretability vs. Precision:\n",
    "\n",
    "Ridge may be preferred when maintaining interpretability of all features is crucial, as it tends to keep all features in the \n",
    "model.\n",
    "Lasso may be favored when interpretability is less critical, and the goal is to identify a subset of the most important \n",
    "features.\n",
    "\n",
    "Trade-Offs and Limitations:\n",
    "\n",
    "Lack of Uniqueness: Ridge and Lasso regularization may not always yield unique solutions. Different sets of coefficients \n",
    "can provide similar regularization terms, and the choice between them might depend on optimization algorithms or specific \n",
    "implementations.\n",
    "\n",
    "Sensitivity to Feature Scaling: Both Ridge and Lasso are sensitive to the scale of features. It's important to scale features\n",
    "properly before applying regularization to ensure fair treatment of all features.\n",
    "\n",
    "Data-Dependent Performance: The choice between Ridge and Lasso, as well as the optimal α value, is often data-dependent. \n",
    "Cross-validation or other tuning methods are commonly used to find the best hyperparameter values for a specific dataset.\n",
    "\n",
    "Trade-Off Between Bias and Variance: Increasing α in Ridge or Lasso generally reduces model complexity, introducing more \n",
    "bias but potentially reducing variance. The optimal balance depends on the bias-variance trade-off relevant to the specific \n",
    "problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
