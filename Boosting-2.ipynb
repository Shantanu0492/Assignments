{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40327e5e",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183a6e76",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression or Gradient Boosting, is a powerful machine learning technique used for regression tasks. It's an ensemble learning method that combines the predictions of multiple weak learners (usually decision trees) to create a strong regression model. Gradient Boosting is known for its high predictive accuracy and ability to handle complex relationships between features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104bfc9c",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b187d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40f1b398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 5), (1000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = make_regression(n_samples=1000,n_features=5,n_informative=3,random_state=42,shuffle=False)\n",
    "\n",
    "X.shape , y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5918d9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((750, 5), (250, 5), (750,), (250,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "\n",
    "X_train.shape , X_test.shape , y_train.shape , y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0574b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared : 0.9897602727854303\n",
      "Mean Square Error : 28.72800275208031\n",
      "Mean Absolute Error : 4.09290914102256\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingRegressor()\n",
    "gb.fit(X_train,y_train)\n",
    "y_pred = gb.predict(X_test)\n",
    "print(f\"R-squared : {r2_score(y_test,y_pred)}\")\n",
    "print(f\"Mean Square Error : {mean_squared_error(y_test,y_pred)}\")\n",
    "print(f\"Mean Absolute Error : {mean_absolute_error(y_test,y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7019c7",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d368a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate':[0.01,0.1,0.5,1] ,\n",
    "          'n_estimators': [100,200,300],\n",
    "          'max_depth': [2,3,5]\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecf9a4f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13724/3292322565.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGradientBoostingRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"r2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "clf = GridSearchCV(GradientBoostingRegressor(),param_grid = params,cv=5,scoring=\"r2\")\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc4f764e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 300}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "660f6723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.990703793295617"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44eff76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared : 0.99338315831917\n",
      "Mean Square Error : 18.56383886345091\n",
      "Mean Absolute Error : 3.375041972273857\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingRegressor(**clf.best_params_)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"R-squared : {r2_score(y_test,y_pred)}\")\n",
    "print(f\"Mean Square Error : {mean_squared_error(y_test,y_pred)}\")\n",
    "print(f\"Mean Absolute Error : {mean_absolute_error(y_test,y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa731f",
   "metadata": {},
   "source": [
    "### Observation\n",
    "- On performing Hyperparameter tuning on the same dataset we obtained an increase in r2 score from 98.9 to 99.3\n",
    "- The best hyperparameters are 'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 300\n",
    "- There is a significant decrease in MSE and MAE after performing hyper paramter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7278bf56",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd921c",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner, also known as a base learner or base model, is a simple and relatively low-performing machine learning model that is used as a building block in the ensemble. Weak learners are typically decision trees with limited depth, often referred to as \"stumps\" or \"shallow trees.\" \n",
    "\n",
    "The key characteristics of a weak learner are:\n",
    "\n",
    "1. ***Weak learners are intentionally kept simple and have limited complexity. For decision trees, this means they are shallow and have only a few levels or nodes.***\n",
    "\n",
    "2. ***Weak learners individually may not perform well on the training data. They have limited ability to capture complex relationships in the data.***\n",
    "\n",
    "3. ***Weak learners are designed to be biased towards the errors made by the current ensemble of models. They focus on the examples that are challenging to classify or predict correctly.***\n",
    "\n",
    "The role of weak learners in Gradient Boosting is crucial. The algorithm combines the predictions of multiple weak learners in a sequential manner, with each learner aiming to correct the errors or residuals made by the previous ensemble. The cumulative effect of adding multiple weak learners is that the ensemble becomes a strong learner that can capture complex patterns and achieve high predictive accuracy.\n",
    "\n",
    "Gradient Boosting iteratively fits a weak learner to the residuals of the previous ensemble. The weak learner's job is to find patterns in the data that the ensemble has not yet captured. These patterns typically correspond to the remaining errors in the predictions. By focusing on the most challenging examples, the ensemble gradually reduces the errors and improves its overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb1035",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7378752f",
   "metadata": {},
   "source": [
    "- Gradient Boosting is an ensemble learning technique that combines the predictions of multiple weak learners (usually shallow decision trees) to create a strong and accurate model. The idea is that by combining the outputs of these weak learners, the ensemble can overcome the limitations of individual models and capture complex patterns in the data.\n",
    "\n",
    "- Gradient Boosting builds the ensemble sequentially, with each weak learner focusing on correcting the errors made by the previous ensemble. In other words, it learns from the mistakes of the ensemble and gradually reduces those mistakes.\n",
    "\n",
    "- The \"Gradient\" in Gradient Boosting refers to the gradient descent optimization technique used to minimize a loss function. In each iteration, a weak learner is trained to minimize the loss function with respect to the residuals (errors) made by the current ensemble. This means the weak learner is \"guided\" by the gradient of the loss function to improve the predictions.\n",
    "\n",
    "-  After training, the predictions from the weak learner are combined with the predictions from the previous ensemble, and each contribution is weighted. The weights are determined by the optimization process and are based on the weak learner's ability to reduce the errors. This weighted combination ensures that the ensemble focuses more on the examples that are challenging to predict.\n",
    "\n",
    "- Gradient Boosting includes regularization techniques, such as the learning rate, to control the step size and prevent overfitting. The learning rate determines how much each weak learner's prediction is added to the ensemble's predictions. Smaller learning rates lead to more conservative updates and reduce the risk of overfitting.\n",
    "\n",
    "- The iterative nature of Gradient Boosting allows it to adapt and improve its predictions over time. It continues to add weak learners until a stopping criterion is met, such as reaching a maximum number of iterations or achieving a satisfactory level of performance. The final ensemble is a strong learner capable of achieving high predictive accuracy.\n",
    "\n",
    "In essence, Gradient Boosting is a \"learning from mistakes\" approach that builds a strong model by iteratively correcting the errors of the previous ensemble. It leverages the power of many weak learners to create a robust and accurate predictive model, making it one of the most popular and effective machine learning algorithms for a wide range of tasks, including regression and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a4a9e",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b179835",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. The steps include :\n",
    "\n",
    "1. **Initialization:**\n",
    "   - The process starts with an initial prediction, often a simple one like the mean of the target variable for regression problems or the class with the highest frequency for classification problems. This initial prediction serves as the starting point for building the ensemble.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Iteration (Sequential Construction):**\n",
    "   - In each iteration, a new weak learner (often a shallow decision tree, also called a \"stump\") is trained on the training data.\n",
    "   - The weak learner's primary task is to fit the residuals (the differences between the true target values and the current ensemble's predictions) from the previous iteration. These residuals represent the errors made by the current ensemble.\n",
    "   - The weak learner is trained to minimize the residuals' loss function, effectively learning to correct the ensemble's errors.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **Weighted Contribution:**\n",
    "   - Once the new weak learner is trained, its predictions are combined with the predictions from the previous ensemble. However, these contributions are weighted.\n",
    "   - The weights are determined by an optimization process, where the algorithm finds the optimal weight for each weak learner. The optimization aims to minimize a loss function that quantifies how well the ensemble is performing.\n",
    "\n",
    "<br>\n",
    "\n",
    "4. **Update Ensemble Predictions:**\n",
    "   - The predictions from the new weak learner, weighted according to their importance, are added to the ensemble's predictions from the previous iterations.\n",
    "   - This update improves the ensemble's predictions, gradually reducing the errors made on the training data.\n",
    "\n",
    "<br>\n",
    "\n",
    "5. **Iterative Process:**\n",
    "   - Steps 2 to 4 are repeated for a fixed number of iterations (controlled by the `n_estimators` hyperparameter) or until a stopping criterion is met. The stopping criterion can be based on the magnitude of residuals or other performance metrics.\n",
    "\n",
    "<br>\n",
    "\n",
    "6. **Final Ensemble:**\n",
    "   - The final ensemble is the cumulative effect of all the weak learners' contributions. It represents the ensemble's predictions after all iterations.\n",
    "\n",
    "<br>\n",
    "\n",
    "7. **Regularization (Learning Rate):**\n",
    "   - Gradient Boosting often includes a regularization parameter called the \"learning rate\" (λ or α). The learning rate controls the step size of each update, ensuring that the ensemble learning process is not too aggressive. Smaller learning rates lead to more conservative updates and help prevent overfitting.\n",
    "\n",
    "<br>\n",
    "\n",
    "The key idea behind Gradient Boosting is to iteratively improve the ensemble's predictions by adding weak learners that focus on correcting the errors made by the previous ensemble. By emphasizing the examples that are challenging to predict, Gradient Boosting creates a strong learner capable of capturing complex relationships in the data and achieving high predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433789f1",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9025dccc",
   "metadata": {},
   "source": [
    "Gradient Boosting is a popular machine learning algorithm used for both regression and classification tasks. \n",
    "\n",
    "The steps involved in constructing the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "1. ***Define the problem: Define the problem you want to solve using Gradient Boosting, whether it's a regression or classification task.***\n",
    "\n",
    "2. ***Define the loss function: The loss function is a measure of how well the algorithm is doing in fitting the training data. In Gradient Boosting, we typically use a differentiable loss function such as mean squared error for regression or log loss for classification.***\n",
    "\n",
    "3. ***Create an initial model: Create an initial model to make predictions. This model can be as simple as the mean of the target variable or a linear regression model.***\n",
    "\n",
    "4. ***Calculate the residual errors: Calculate the residual errors by subtracting the predictions of the initial model from the actual values of the target variable.***\n",
    "\n",
    "5. ***Train a new model on the residual errors: Train a new model on the residual errors from the previous step. This model is usually a decision tree with a fixed depth.***\n",
    "\n",
    "6. ***Add the predictions of the new model to the previous predictions: Add the predictions of the new model to the previous predictions to update the model. This process is called boosting because we are boosting the performance of the model by adding new models to it.***\n",
    "\n",
    "7. ***Repeat steps 4 to 6 until convergence: Repeat steps 4 to 6 until the model converges or until a stopping criterion is met. The stopping criterion can be a maximum number of models, a threshold for the improvement of the loss function, or a maximum depth for the decision trees.***\n",
    "\n",
    "8. ***Make predictions: Use the final model to make predictions on new data.***\n",
    "\n",
    "Overall, Gradient Boosting works by iteratively improving the model by adding new models to correct the errors of the previous models. This process continues until the model converges or a stopping criterion is met."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
